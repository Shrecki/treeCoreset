//
// Created by guibertf on 9/22/21.
//

#ifndef UNTITLED_KMEANSPLUSPLUS_H
#define UNTITLED_KMEANSPLUSPLUS_H

#include "Point.h"
#include <vector>
#include <cmath>

/**
 * Utility class to pass around centroids, point assignments and total cost of the chosen solution
 */
class Threeple{
public:
    std::vector<Eigen::VectorXd> points;
    std::vector<unsigned int> assignments;
    double totalCost;
    Threeple(std::vector<Eigen::VectorXd> &points, std::vector<unsigned int> &assignments, double totalCost):
    totalCost(totalCost){
        this->points = std::move(points);
        this->assignments = std::move(assignments);
    }
};

namespace kmeans {

    /**
     * @brief Find nearest cluster index in provided centroid vector, leveraging triangular ineq. to minimize distance computations.
     *  The minimization comes from lemma 1 of Elkan's paper, which states that given three points, a,b,c
     *  if distance(b,c) >= 2*distance(a,b) then distance(a,c) >= distance(a,b)
     *  In other words, under specific conditions, we can avoid computing distance(a,c).
     *  Obviously, it means remembering distance(a,b), which is nothing more than the distance between the point and its currently assigned centroid.
     *  The distance between b and c is then the distance between current centroid and candidate centroid, ie:
     *  if distance(current centroid, candidate centroid) >= 2*distance(point, current centroid) then we know the candidate is farther away than
     *  the current centroid, so no need to compute any distance.
     *  If this condition is not satisfied, then we compute the distance normally.
     * @cite Elkan, Charles. "Using the triangle inequality to accelerate k-means." Proceedings of the 20th international conference on Machine Learning (ICML-03). 2003.
     * @param centroids
     * @param point
     * @return Index of the centroid nearest to provided point
     */
    int findNearestClusterIndex(const std::vector<Eigen::VectorXd> &centroids, const Eigen::VectorXd &point);

    /**
     * @brief Generates start centroids. The first centroid is chosen randomly, the others as a function of their distance then.
     * This code is an adaptation of https://github.com/ieyjzhou/KmeansPlusPlus , which was for 2D points while this implementation
     * handles points of any dimension, and is to implement start centroids of the kmeans++ algorithm.
     * @see https://github.com/ieyjzhou/KmeansPlusPlus
     * @cite Arthur, David, and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Stanford, 2006.
     * @param inputPoints
     * @param k
     * @return
     */
    std::vector<Eigen::VectorXd> generateStartCentroids(const Eigen::VectorXd &firstCentroid, const std::vector<Point *> &inputPoints, const unsigned int &k);

        /**
         * @brief k-means algorithm, with the potential to specify initial startCentroids (otherwise it is generated by taking k points randomly)
         * The algorithm stops after specified epochs or early if no new point assignment occurred in an iteration.
         * To minimize the number of distance computations performed, it relies on Elkan's algorithm to prune away some distance checks based on triangle inequality.
         * @cite Elkan, Charles. "Using the triangle inequality to accelerate k-means." Proceedings of the 20th international conference on Machine Learning (ICML-03). 2003.
         * @see findNearestClusterIndex
         * @param inputPoints
         * @param startCentroids
         * @param k
         * @param epochs number of epochs before algorithm finishes
         * @return Threeple containing centroids, point assignments and total cost of this solution
         */
    Threeple *kMeans(const std::vector<Point *> &inputPoints, const std::vector<Eigen::VectorXd> *startCentroids, const unsigned int &k, const unsigned int &epochs);


    /**
     * The k-means++ algorithm, as described in k-means++: The Advantages of Careful Seeding
     * First, initialize the centroids with a specific seeding
     * Then simply run the usual k-means
     * @cite Arthur, David, and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Stanford, 2006.
     * @see generateStartCentroids
     * @see kMeans
     * @param inputPoints
     * @param k
     * @param epochs
     * @return Threeple, composed of cluster centroids, point assignments (in the form of indices denoting belonging to specific centroid, starting at 0) and total cost of this solution.
     */
    Threeple *kMeansPlusPlus(const std::vector<Point *> &inputPoints, const unsigned int &k, const unsigned int &epochs);


    /**
     * @brief Runs k-means++ nTries times, keeping track of total cost (defined as sum of distances of points to their final cluster). Returns clusters with lowest total cost.
     * The reason why it would be desirable to re-run kmeans++ is that cluster initialization is still stochastic and we have no guarantee it be the optimal one.
     * We increase our chances, by sampling our solution space several times, to reach an overall better solution.
     * @see kMeansPlusPlus
     * @param nTries
     * @param inputPoints
     * @param k
     * @param epochs
     * @return Cluster centroids corresponding to kmeans++ with lowest cost
     */
    std::vector<Eigen::VectorXd> getBestClusters(int nTries, const std::vector<Point *> &inputPoints, const unsigned int &k, const unsigned int &epochs);

    static void convertFromVectorOfEigenXdToArray(std::vector<double> &outputData, const std::vector<Eigen::VectorXd> &vectors){
        int nClusters = vectors.size();
        int dimsPerCluster = vectors.at(0).size();

        outputData.reserve(vectors.size()*vectors.at(0).size());

        for(int i = 0; i < nClusters ; ++i){
            for(int j = 0; j < vectors.at(0).size(); ++j){
                outputData.push_back(vectors.at(i)[j]);
            }
        }
    }
}

#endif //UNTITLED_KMEANSPLUSPLUS_H
